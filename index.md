\documentclass[12pt]{article}
\usepackage{ctex}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{fancyhdr}

\geometry{a4paper,left=2.5cm,right=2.5cm,top=3.5cm,bottom=2.5cm}
\date{}
\begin{document}
\title{Reading Report of SUNA, VAE and CNNs}
\maketitle
\setlength{\parindent}{0pt}
\renewcommand{\baselinestretch}{1.25}
\section{Spectrum-Diverse Neuroevolution with Unified Neural Models}
\subsection{overview}
this paper structure a kind of neural network which add time scales, Inhibition and excitation of neurons(control signal, which will be solely
responsible for activating and deactivating neurons), heterosynaptic mechanisms, feedback to basic multi-layer neural. In this paper, writer change sigmoid function into 
$$Ins_t = Ins_{t−1}+ (1/{adaptationSpeed})*(a − Ins_{t−1})$$
$$y = Inst\eqno(1.1)$$
this function take adaptationSpeed as parameter to update its previous internal state $Ins_{t−1}$, which designed for changing the neuron output completely after adaptationSpeed iterations if
if the neuron input changes.
Also, writer divided Spectrum into the number of identity neurons, sigmoid neurons, threshold neurons, random neurons, control neurons, slower neurons. \\
The writer uses these technologies to determine whether control neurons are activated, and change the network structure without adding or deleting notes and connections.\\
\subsection{comparation SOM with Novelty map}
1.SOM
$$W_j=W_j/{\Vert{W_j}\Vert}, X=X/{\Vert{X}\Vert}$$
$$s=\mathop{argmax}\vert{W_j^T*X}\vert\eqno(1.2)$$
2.Novelty map
$$U=S \backslash \{ a_i\}$$
$$uniqueness=\mathop{min}_{a_k\in{U}}(dist(a_i,a_k))\eqno(1.3)$$
\subsection{stiil need to read}
1.TWEANNs 2.github 3. novelty map [41] (Section VI-D)
\section{Variational Autoencoder}
\subsection{overview}
an unsupervised manner to learn a encoding for a set of data.
Variational autoencoder models inherit the autoencoder architecture, but make strong assumptions concerning the distribution of latent variables. They use a variational approach for latent representation learning, which results in an additional loss component and a specific estimator for the training algorithm called the Stochastic Gradient Variational Bayes (SGVB) estimator.It assumes that the data is generated by a directed graphical model $ p(X|z)$ and that the encoder is learning an approximation $ q_{\phi}(z|X)$ to the posterior distribution$ p_{\theta}(X|z)$  
$$p(z|X)= \dfrac{p(X|z)*p(z)}{\int{p(X|z)*p(z)}dz}$$
$${KL(q(z)||p(z|X))=\int{q(z)\log{\frac{q(z)}{p(z|X)}}}dz=\int{q(z)[\log{q(z)}-\log{p(z|X)}]}dz$$
$$=\int{q(z)[\log{q(z)}-\log{p(X|z)}-\log{p(z)}]dz+\log{p(X)}$$
$$\log{p(X)}-KL(q(z)||p(z|X)=\int{q(z)\log{p(X|z)dz-KL(q(z)||p(z))}}\eqno(2.1)$$
in this way MAX \{$\int{q(z)\log{p(X|z)dz$\}, and MIN \{$KL(q(z)||p(z))$\}
\subsection{stiil need to read}
1.GAN 
\section{CNNs}
I read the paper about Convolutional Neural Networks (LeNet) and want to make it come true in Theano. But I still confused by the number of filters. Is it adjustment parameters personally?
\section{My questions and opinion}
1.I wonder why spectrum is composed of the following
properties:
1) the number of identity neurons;
2) the number of sigmoid neurons;
3) the number of threshold neurons;
4) the number of random neurons;
5) the number of control neurons;
6) the number of slower neurons;
why the sigmoid neurons and threshold neurons are Separated, if these two kinds of neurons are different, How about tanh and Relu, why the number of those can not be a part of spectrum.
\\
2. why use Novelty Map as method of classfication. Why not use SOM 
or  KNN.
\\
3. I think I still need  to read much more books and papers since many concepts in the paper I have not known clearly, like TWEANN and NEAT. After reading all these basic knowledge, I would like to read SUNA again and look at its GitHub
\\
\\
\\
\\
\date{2019.3.19}
\end{document}
